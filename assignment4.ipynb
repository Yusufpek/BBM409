{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8318f7a3",
   "metadata": {},
   "source": [
    "# Project Assignment 4 : Image Classification System for Indian Bird Species\n",
    "\n",
    "**Name/Group ID:** Berke Yusuf Uğurlu - Yusuf İpek (Group 6)\n",
    "\n",
    "**Date:** 01.06.2025\n",
    "\n",
    "**Course:** BBM 409 - Machine Learning Laboratory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e51d9e",
   "metadata": {},
   "source": [
    "## 1. Dataset Setup\n",
    "\n",
    "The dataset for this project contains twenty-five species of birds in India, with 1,500 samples for each species. In total, there are 37,500 images, each approximately 1 MP. The original samples are split into 1,200 for training and 300 for validation. For this assignment, we need to further split each original validation set into half randomly to create a test set and a new validation set, each comprising 150 samples per species. This will achieve an 80% training, 10% testing, and 10% validation split.\n",
    "\n",
    "**Dataset Source:** [Kaggle Indian Birds Species Image Classification Dataset](https://www.kaggle.com/datasets/ichhadhari/indian-birds/data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d57f8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.19.0\n",
      "NumPy Version: 1.26.4\n",
      "OpenCV Version: 4.11.0\n"
     ]
    }
   ],
   "source": [
    "# General Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import cv2 # OpenCV for image processing\n",
    "from PIL import Image # Pillow for image processing\n",
    "import random\n",
    "import shutil # For file operations\n",
    "\n",
    "# Deep Learning Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.applications import VGG16, ResNet50, EfficientNetB0\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"NumPy Version:\", np.__version__)\n",
    "print(\"OpenCV Version:\", cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b922140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Dataset directory not found at ./indian-birds/birds\n"
     ]
    }
   ],
   "source": [
    "# --- Dataset Path Configuration ---\n",
    "BASE_DATA_DIR = \"Birds_25\"\n",
    "\n",
    "# Check if the dataset path exists\n",
    "if not os.path.exists(BASE_DATA_DIR):\n",
    "    print(f\"ERROR: Dataset directory not found at {BASE_DATA_DIR}\")\n",
    "else:\n",
    "    print(f\"Dataset directory found at: {BASE_DATA_DIR}\")\n",
    "    # Example: List species to confirm\n",
    "    try:\n",
    "        species_list = sorted([d for d in os.listdir(BASE_DATA_DIR) if os.path.isdir(os.path.join(BASE_DATA_DIR, d))])\n",
    "        if not species_list: \n",
    "            train_path_check = os.path.join(BASE_DATA_DIR, 'train')\n",
    "            if os.path.exists(train_path_check):\n",
    "                 species_list = sorted([d for d in os.listdir(train_path_check) if os.path.isdir(os.path.join(train_path_check, d))])\n",
    "        print(f\"Found {len(species_list)} species (folders). First few: {species_list[:5]}\")\n",
    "        if not species_list:\n",
    "             print(\"Could not automatically determine species list. Please check dataset structure.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing dataset subdirectories: {e}\")\n",
    "\n",
    "# Define output directories for our custom splits\n",
    "OUTPUT_TRAIN_DIR = \"./dataset_split/train\"\n",
    "OUTPUT_VAL_DIR = \"./dataset_split/validation\"\n",
    "OUTPUT_TEST_DIR = \"./dataset_split/test\"\n",
    "\n",
    "# Create these directories if they don't exist\n",
    "os.makedirs(OUTPUT_TRAIN_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_VAL_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_TEST_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531546c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Splitting Function ---\n",
    "def create_splits(base_dir, output_train_dir, output_val_dir, output_test_dir):\n",
    "    \"\"\"\n",
    "    Splits the dataset according to the assignment's 80-10-10 ratio.\n",
    "    Original structure: dataset/train/<species>/ (1200 images)\n",
    "                        dataset/valid/<species>/ (300 images)\n",
    "    New structure:  output_train_dir/<species>/ (1200 images)\n",
    "                    output_val_dir/<species>/ (150 images)\n",
    "                    output_test_dir/<species>/ (150 images)\n",
    "    \"\"\"\n",
    "    print(\"Starting dataset splitting...\")\n",
    "    # Kaggle dataset structure is often /kaggle/input/dataset_name/\n",
    "    # The provided link has train, test, valid folders at the root of \"birds\".\n",
    "    original_train_path = os.path.join(base_dir, \"train\")\n",
    "    original_valid_path = os.path.join(base_dir, \"valid\") \n",
    "\n",
    "    if not os.path.exists(original_train_path) or not os.path.exists(original_valid_path):\n",
    "        print(f\"Error: Could not find 'train' or 'valid' subdirectories in {base_dir}.\")\n",
    "        print(\"Please ensure your BASE_DATA_DIR is set correctly and the dataset has the expected structure.\")\n",
    "        return\n",
    "\n",
    "    all_species = sorted([d for d in os.listdir(original_train_path) if os.path.isdir(os.path.join(original_train_path, d))])\n",
    "    if not all_species:\n",
    "        print(f\"No species subdirectories found in {original_train_path}. Splitting cannot proceed.\")\n",
    "        return\n",
    "\n",
    "    for species_name in all_species:\n",
    "        print(f\"Processing species: {species_name}\")\n",
    "\n",
    "        # Create species directories in output\n",
    "        os.makedirs(os.path.join(output_train_dir, species_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_val_dir, species_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_test_dir, species_name), exist_ok=True)\n",
    "\n",
    "        # 1. Handle Training Data (1200 samples)\n",
    "        species_train_path_original = os.path.join(original_train_path, species_name)\n",
    "        train_images = [f for f in os.listdir(species_train_path_original) if os.path.isfile(os.path.join(species_train_path_original, f))]\n",
    "        for img_name in train_images:\n",
    "            shutil.copy(os.path.join(species_train_path_original, img_name),\n",
    "                        os.path.join(output_train_dir, species_name, img_name))\n",
    "\n",
    "        # 2. Handle Original Validation Data (300 samples) and split into new validation and test\n",
    "        species_valid_path_original = os.path.join(original_valid_path, species_name)\n",
    "        original_val_images = [f for f in os.listdir(species_valid_path_original) if os.path.isfile(os.path.join(species_valid_path_original, f))]\n",
    "        random.shuffle(original_val_images) # Shuffle for random split\n",
    "\n",
    "        # Split into 150 for new validation and 150 for test\n",
    "        new_val_set = original_val_images[:150]\n",
    "        new_test_set = original_val_images[150:300] # Assuming exactly 300 files\n",
    "\n",
    "        for img_name in new_val_set:\n",
    "            shutil.copy(os.path.join(species_valid_path_original, img_name),\n",
    "                        os.path.join(output_val_dir, species_name, img_name))\n",
    "\n",
    "        for img_name in new_test_set:\n",
    "            shutil.copy(os.path.join(species_valid_path_original, img_name),\n",
    "                        os.path.join(output_test_dir, species_name, img_name))\n",
    "\n",
    "    print(\"Dataset splitting completed.\")\n",
    "    print(f\"Training data: {OUTPUT_TRAIN_DIR}\")\n",
    "    print(f\"Validation data: {OUTPUT_VAL_DIR}\")\n",
    "    print(f\"Test data: {OUTPUT_TEST_DIR}\")\n",
    "\n",
    "# --- Check if splitting is already done to avoid re-doing it ---\n",
    "# A simple check: if the output directories have species subfolders.\n",
    "# More robust check would be to count files.\n",
    "# For this script, we'll assume if the first species folder exists in train, it's likely done.\n",
    "# You might want to clear the output_split folders if you need to re-run.\n",
    "\n",
    "# Check if base_dir exists and has content before attempting to list species\n",
    "species_list_for_check = []\n",
    "if os.path.exists(os.path.join(BASE_DATA_DIR, \"train\")):\n",
    "    try:\n",
    "        species_list_for_check = sorted([d for d in os.listdir(os.path.join(BASE_DATA_DIR, \"train\")) if os.path.isdir(os.path.join(BASE_DATA_DIR, \"train\", d))])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Could not list species in {os.path.join(BASE_DATA_DIR, 'train')}. Skipping split check.\")\n",
    "\n",
    "if species_list_for_check and not os.path.exists(os.path.join(OUTPUT_TRAIN_DIR, species_list_for_check[0])):\n",
    "    if os.path.exists(BASE_DATA_DIR) and os.path.exists(os.path.join(BASE_DATA_DIR, \"train\")) and os.path.exists(os.path.join(BASE_DATA_DIR, \"valid\")):\n",
    "        create_splits(BASE_DATA_DIR, OUTPUT_TRAIN_DIR, OUTPUT_VAL_DIR, OUTPUT_TEST_DIR)\n",
    "    else:\n",
    "        print(\"Original dataset structure ('train', 'valid' folders inside BASE_DATA_DIR) not found. Skipping automatic split.\")\n",
    "        print(f\"Please ensure your data is correctly placed in {BASE_DATA_DIR} or manually split into:\")\n",
    "        print(f\"  {OUTPUT_TRAIN_DIR}\")\n",
    "        print(f\"  {OUTPUT_VAL_DIR}\")\n",
    "        print(f\"  {OUTPUT_TEST_DIR}\")\n",
    "else:\n",
    "    if species_list_for_check:\n",
    "        print(\"Split dataset directories seem to exist. Skipping splitting process.\")\n",
    "    else:\n",
    "        print(\"Cannot determine if dataset is split as original species list could not be obtained. Please check paths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d60fda1",
   "metadata": {},
   "source": [
    "### 1.1. Data Loading and Preprocessing Utilities\n",
    "We will define functions to load image paths and labels, and a basic image preprocessing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a9c4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "IMG_CHANNELS = 3 # For color images\n",
    "\n",
    "def load_image_paths_and_labels(data_dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    class_names = sorted(os.listdir(data_dir))\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(class_names) # Fit on all class names\n",
    "\n",
    "    for class_name in class_names:\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                image_paths.append(img_path)\n",
    "                labels.append(class_name)\n",
    "    \n",
    "    numeric_labels = label_encoder.transform(labels)\n",
    "    return image_paths, numeric_labels, label_encoder, class_names\n",
    "\n",
    "def preprocess_image(image_path, target_size=(IMG_WIDTH, IMG_HEIGHT), normalize=True):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('RGB') # Ensure 3 channels\n",
    "        img = img.resize(target_size)\n",
    "        img_array = np.array(img)\n",
    "        if normalize:\n",
    "            img_array = img_array / 255.0 # Normalize to [0, 1]\n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None # Return None if an image is corrupted or cannot be processed\n",
    "\n",
    "# Load data for traditional ML parts\n",
    "\n",
    "print(\"Loading image paths and labels for train, validation, and test sets...\")\n",
    "try:\n",
    "    X_train_paths, y_train_raw, label_encoder, class_names = load_image_paths_and_labels(OUTPUT_TRAIN_DIR)\n",
    "    X_val_paths, y_val_raw, _, _ = load_image_paths_and_labels(OUTPUT_VAL_DIR)\n",
    "    X_test_paths, y_test_raw, _, _ = load_image_paths_and_labels(OUTPUT_TEST_DIR)\n",
    "\n",
    "    print(f\"Training samples: {len(X_train_paths)}, Validation samples: {len(X_val_paths)}, Test samples: {len(X_test_paths)}\")\n",
    "    print(f\"Number of classes: {len(class_names)}\")\n",
    "    print(f\"Example class names: {class_names[:5]}\")\n",
    "\n",
    "    # Save the label encoder and class names for later use\n",
    "    np.save('label_encoder_classes.npy', label_encoder.classes_)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during loading image paths: {e}\")\n",
    "    print(\"Please ensure the dataset has been split correctly into train, validation, and test directories.\")\n",
    "    X_train_paths, y_train_raw, X_val_paths, y_val_raw, X_test_paths, y_test_raw, class_names = [], [], [], [], [], [], []\n",
    "\n",
    "\n",
    "# Function to load a subset of data\n",
    "def load_subset_paths(image_paths, labels, subset_fraction=0.1, random_state=42):\n",
    "    if subset_fraction >= 1.0:\n",
    "        return image_paths, labels\n",
    "    \n",
    "    # Stratified split to maintain class proportions\n",
    "    _, sub_paths, _, sub_labels = train_test_split(\n",
    "        image_paths, labels, \n",
    "        test_size=subset_fraction, \n",
    "        random_state=random_state, \n",
    "        stratify=labels\n",
    "    )\n",
    "    print(f\"Using a subset of {len(sub_paths)} samples ({subset_fraction*100:.1f}% of original).\")\n",
    "    return sub_paths, sub_labels\n",
    "\n",
    "\n",
    "USE_SUBSET = False # Set to True to use a subset\n",
    "SUBSET_FRACTION = 0.1 # 10% of the data\n",
    "\n",
    "if USE_SUBSET and X_train_paths: # Check if X_train_paths is populated\n",
    "    print(\"Loading a subset of the data for development...\")\n",
    "    X_train_paths, y_train_raw = load_subset_paths(X_train_paths, y_train_raw, SUBSET_FRACTION)\n",
    "    X_val_paths, y_val_raw = load_subset_paths(X_val_paths, y_val_raw, SUBSET_FRACTION)\n",
    "    X_test_paths, y_test_raw = load_subset_paths(X_test_paths, y_test_raw, SUBSET_FRACTION)\n",
    "    print(f\"Subset - Training samples: {len(X_train_paths)}, Validation samples: {len(X_val_paths)}, Test samples: {len(X_test_paths)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
